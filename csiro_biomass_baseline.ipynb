{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSIRO Pasture Biomass Baseline\n",
        "\n",
        "This notebook builds a reproducible baseline for the CSIRO biomass prediction challenge. It is designed to run end-to-end on Kaggle, assuming the competition data is available under `/kaggle/input/csiro-biomass/`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook Roadmap\n",
        "\n",
        "1. Imports and configuration\n",
        "2. Load the tabular data and inspect it briefly\n",
        "3. Feature engineering helpers\n",
        "4. Train/validation split with cross-validation diagnostics\n",
        "5. Fit the final model on full data\n",
        "6. Generate the Kaggle submission file\n",
        "7. (Optional) Visualise a sample pasture image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.base import clone\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "pd.set_option(\"display.max_columns\", 50)\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "DATA_DIR = Path(\"/kaggle/input/csiro-biomass\")\n",
        "TRAIN_CSV = DATA_DIR / \"train.csv\"\n",
        "TEST_CSV = DATA_DIR / \"test.csv\"\n",
        "SAMPLE_SUB_CSV = DATA_DIR / \"sample_submission.csv\"\n",
        "TRAIN_IMAGE_SAMPLE = DATA_DIR / \"train\" / \"ID1011485656.jpg\"\n",
        "\n",
        "if not TRAIN_CSV.exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"Expected competition data under /kaggle/input/csiro-biomass/. \"\n",
        "        \"Please add the dataset to the notebook before running it.\"\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df = pd.read_csv(TEST_CSV)\n",
        "sample_submission = pd.read_csv(SAMPLE_SUB_CSV)\n",
        "\n",
        "print(f\"Train rows: {len(train_df):,}\")\n",
        "print(f\"Test rows: {len(test_df):,}\")\n",
        "print(f\"Sample submission rows: {len(sample_submission):,}\")\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_summary = (\n",
        "    train_df.describe(include=\"all\", datetime_is_numeric=True)\n",
        "    .transpose()\n",
        ")\n",
        "train_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_ratio = train_df.isna().mean().sort_values(ascending=False)\n",
        "missing_ratio[missing_ratio > 0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering\n",
        "\n",
        "We will keep the baseline focused on the tabular metadata. Each sample contains five target measurements. The steps below pivot the training targets into a wide format and create simple date-derived features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TARGET_COLUMNS = [\n",
        "    \"Dry_Green_g\",\n",
        "    \"Dry_Dead_g\",\n",
        "    \"Dry_Clover_g\",\n",
        "    \"GDM_g\",\n",
        "    \"Dry_Total_g\",\n",
        "]\n",
        "META_COLUMNS = [\n",
        "    \"sample_id\",\n",
        "    \"image_path\",\n",
        "    \"Sampling_Date\",\n",
        "    \"State\",\n",
        "    \"Species\",\n",
        "    \"Pre_GSHH_NDVI\",\n",
        "    \"Height_Ave_cm\",\n",
        "]\n",
        "\n",
        "def prepare_metadata(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    meta = (\n",
        "        df[META_COLUMNS]\n",
        "        .drop_duplicates(subset=[\"sample_id\"])\n",
        "        .set_index(\"sample_id\")\n",
        "    )\n",
        "    meta[\"Sampling_Date\"] = pd.to_datetime(meta[\"Sampling_Date\"], errors=\"coerce\")\n",
        "    meta[\"sampling_year\"] = meta[\"Sampling_Date\"].dt.year\n",
        "    meta[\"sampling_month\"] = meta[\"Sampling_Date\"].dt.month\n",
        "    meta[\"sampling_dayofyear\"] = meta[\"Sampling_Date\"].dt.dayofyear\n",
        "    meta = meta.drop(columns=[\"Sampling_Date\", \"image_path\"], errors=\"ignore\")\n",
        "    return meta\n",
        "\n",
        "def prepare_targets(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    pivot = df.pivot(index=\"sample_id\", columns=\"target_name\", values=\"target\")\n",
        "    for target in TARGET_COLUMNS:\n",
        "        if target not in pivot.columns:\n",
        "            pivot[target] = np.nan\n",
        "    return pivot[TARGET_COLUMNS]\n",
        "\n",
        "train_meta = prepare_metadata(train_df)\n",
        "train_targets = prepare_targets(train_df)\n",
        "\n",
        "# Align features and targets on the same sample ids\n",
        "common_ids = train_meta.index.intersection(train_targets.index)\n",
        "X = train_meta.loc[common_ids].copy()\n",
        "y = train_targets.loc[common_ids].copy()\n",
        "\n",
        "print(f\"Training samples: {len(X):,}\")\n",
        "X.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_features = [\n",
        "    \"Pre_GSHH_NDVI\",\n",
        "    \"Height_Ave_cm\",\n",
        "    \"sampling_year\",\n",
        "    \"sampling_month\",\n",
        "    \"sampling_dayofyear\",\n",
        "]\n",
        "categorical_features = [\"State\", \"Species\"]\n",
        "\n",
        "one_hot_kwargs = {\"handle_unknown\": \"ignore\"}\n",
        "if \"sparse_output\" in OneHotEncoder.__init__.__code__.co_varnames:\n",
        "    one_hot_kwargs[\"sparse_output\"] = False\n",
        "else:\n",
        "    one_hot_kwargs[\"sparse\"] = False\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", SimpleImputer(strategy=\"median\"), numeric_features),\n",
        "        (\n",
        "            \"cat\",\n",
        "            Pipeline(\n",
        "                steps=[\n",
        "                    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "                    (\"encoder\", OneHotEncoder(**one_hot_kwargs)),\n",
        "                ]\n",
        "            ),\n",
        "            categorical_features,\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "base_regressor = HistGradientBoostingRegressor(\n",
        "    random_state=RANDOM_STATE,\n",
        "    max_depth=None,\n",
        "    learning_rate=0.1,\n",
        "    max_iter=500,\n",
        ")\n",
        "\n",
        "model = Pipeline(\n",
        "    steps=[\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        (\"regressor\", MultiOutputRegressor(base_regressor)),\n",
        "    ]\n",
        ")\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def root_mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "cv_metrics = []\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(cv.split(X), start=1):\n",
        "    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "\n",
        "    fold_model = clone(model)\n",
        "    fold_model.fit(X_train, y_train)\n",
        "    y_pred = fold_model.predict(X_valid)\n",
        "\n",
        "    fold_summary = {\"fold\": fold}\n",
        "    overall_rmse = root_mean_squared_error(y_valid.values, y_pred)\n",
        "    fold_summary[\"overall_rmse\"] = overall_rmse\n",
        "\n",
        "    for idx, target in enumerate(TARGET_COLUMNS):\n",
        "        target_rmse = root_mean_squared_error(y_valid.iloc[:, idx].values, y_pred[:, idx])\n",
        "        fold_summary[f\"rmse_{target}\"] = target_rmse\n",
        "\n",
        "    cv_metrics.append(fold_summary)\n",
        "    print(\n",
        "        f\"Fold {fold}: overall RMSE = {overall_rmse:.2f} \"\n",
        "        + \", \".join(\n",
        "            f\"{target}={fold_summary[f'rmse_{target}']:.2f}\" for target in TARGET_COLUMNS\n",
        "        )\n",
        "    )\n",
        "\n",
        "cv_results = pd.DataFrame(cv_metrics)\n",
        "cv_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_results.mean(numeric_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_model = clone(model)\n",
        "final_model.fit(X, y)\n",
        "\n",
        "test_meta = prepare_metadata(test_df)\n",
        "print(f\"Test samples prepared: {len(test_meta):,}\")\n",
        "\n",
        "test_predictions = final_model.predict(test_meta)\n",
        "prediction_wide = pd.DataFrame(\n",
        "    test_predictions, index=test_meta.index, columns=TARGET_COLUMNS\n",
        ").reset_index()\n",
        "\n",
        "prediction_long = prediction_wide.melt(\n",
        "    id_vars=\"sample_id\", value_vars=TARGET_COLUMNS,\n",
        "    var_name=\"target_name\", value_name=\"target\"\n",
        ")\n",
        "submission = sample_submission.drop(columns=\"target\", errors=\"ignore\").merge(\n",
        "    prediction_long, on=[\"sample_id\", \"target_name\"], how=\"left\"\n",
        ")\n",
        "\n",
        "missing_predictions = submission[\"target\"].isna().sum()\n",
        "if missing_predictions:\n",
        "    raise ValueError(\n",
        "        f\"Submission contains {missing_predictions} missing predictions. \"\n",
        "        \"Check feature preparation for unmatched sample_ids.\"\n",
        "    )\n",
        "\n",
        "submission.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_PATH = Path(\"submission.csv\")\n",
        "submission.to_csv(OUTPUT_PATH, index=False)\n",
        "print(f\"Saved submission to {OUTPUT_PATH.resolve()} with {len(submission):,} rows.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment & Optional GPU Diagnostics\n",
        "\n",
        "Kaggle provides a P100 accelerator for this notebook. The model trains on CPU-only features, so GPU usage is optional. Run the cell below to confirm the GPU is visible if you want to add image models later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualise A Sample Pasture Image\n",
        "\n",
        "Use the cell below to display one of the training images (`ID1011485656.jpg`) so you can inspect the raw data manually.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_IMAGE_SAMPLE.exists():\n",
        "    image = Image.open(TRAIN_IMAGE_SAMPLE)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(TRAIN_IMAGE_SAMPLE.name)\n",
        "else:\n",
        "    print(f\"Image not found at {TRAIN_IMAGE_SAMPLE}. Check the dataset path.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "- Explore richer image features (e.g., CNN embeddings) now that the notebook runs on a GPU.\n",
        "- Try additional regressors (CatBoost, LightGBM, XGBoost) or stacking ensembles.\n",
        "- Engineer agronomic features from dates, species composition, or NDVI deltas.\n",
        "- Log experiments with Weights & Biases or Kaggle notebooks for reproducibility.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
